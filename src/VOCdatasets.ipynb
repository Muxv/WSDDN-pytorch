{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (  # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_small_boxes(boxes, min_size):\n",
    "    \"\"\"Filters out small boxes.\"\"\"\n",
    "    w = boxes[:, 2] - boxes[:, 0]\n",
    "    h = boxes[:, 3] - boxes[:, 1]\n",
    "    mask = (w >= min_size) & (h >= min_size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCAnnotationAnalyzer():\n",
    "    \"\"\"\n",
    "    deal with annotation data (dict)\n",
    "    \n",
    "    Arguments:\n",
    "        cls_to_idx (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not\n",
    "            (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "    def __init__(self, cls_to_idx=None, keep_difficult=False):\n",
    "        self.cls_to_idx = cls_to_idx or dict(zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "        \n",
    "    def __call__(self, annotation: dict):\n",
    "        w = int(annotation['size']['width'])\n",
    "        h = int(annotation['size']['height'])\n",
    "        # if img only contains one gt that annotation['object'] is just a dict, not a list\n",
    "        objects = [annotation['object']] if type(annotation['object']) != list else annotation['object']\n",
    "        res = [] # [xmin, ymin, xmax, ymax, label]\n",
    "        for box in objects:\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            difficult = int(box['difficult'])\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = box['name']\n",
    "            bnd = []\n",
    "            for pt in pts:\n",
    "                bnd.append(int(box['bndbox'][pt]))\n",
    "            bnd.append(self.cls_to_idx[name])\n",
    "            res.append(bnd)\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDectectionDataset(data.Dataset):\n",
    "    def __init__(self, root, year, image_set,\n",
    "                 transform=None, \n",
    "                 target_transform=VOCAnnotationAnalyzer(),\n",
    "                 dataset_name='VOC07_12',\n",
    "                 region_propose='selective_search'):\n",
    "        super(VOCDectectionDataset, self).__init__()\n",
    "        self.datas = datasets.VOCDetection(root, str(year), image_set, download=False)\n",
    "        self.image_set = image_set\n",
    "        self.transform = transform\n",
    "        self.name = dataset_name\n",
    "        self.target_transform = target_transform # use for annotation\n",
    "        self.longer_sides = [480, 576, 688, 864, 1200]\n",
    "        if region_propose not in ['selective_search', 'edge_box']:\n",
    "            raise NotImplementedError(f'{region_propose} not Supported')\n",
    "\n",
    "        self.region_propose = region_propose\n",
    "        self.box_mat = self.get_mat(year, image_set, region_propose)\n",
    "            \n",
    "            \n",
    "    def get_box_from_mat(self, index):\n",
    "        return self.box_mat['boxes'][0][index].tolist()\n",
    "\n",
    "    def get_boxScore_from_mat(self, index):\n",
    "        score = None\n",
    "        if self.region_propose == 'edge_box':\n",
    "            score = self.box_mat['boxScores'][0][index].tolist()\n",
    "        return score\n",
    "    \n",
    "    def get_mat(self, year, image_set, region_propose):\n",
    "        \"\"\"\n",
    "        load the box generated\n",
    "        \"\"\"\n",
    "        boxes = None\n",
    "        boxes_score = None\n",
    "        \n",
    "        if str(year) == '2007' and image_set == 'trainval' and region_propose == 'selective_search':\n",
    "            mat = loadmat(\"../region/SelectiveSearchVOC2007trainval.mat\")\n",
    "        elif str(year) == '2007' and image_set == 'test' and region_propose == 'selective_search':\n",
    "            mat = loadmat(\"../region/SelectiveSearchVOC2007test.mat\")\n",
    "        if str(year) == '2007' and image_set == 'trainval' and region_propose == 'edge_box':\n",
    "            mat = loadmat(\"../region/EdgeBoxesVOC2007trainval.mat\")\n",
    "        elif str(year) == '2007' and image_set == 'test' and region_propose == 'edge_box':\n",
    "            mat = loadmat(\"../region/EdgeBoxesVOC2007test.mat\")\n",
    "        return mat\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        img, gt = self.datas[index]\n",
    "        region = self.get_box_from_mat(index)\n",
    "        region_score = self.get_boxScore_from_mat(index)\n",
    "        if self.target_transform:\n",
    "            gt = self.target_transform(gt[\"annotation\"])\n",
    "        \n",
    "        w, h = img.size\n",
    "        if self.image_set == \"trainval\":\n",
    "            if self.transform is None:\n",
    "                # follow by paper: randomly horiztontal flip and randomly resize\n",
    "                for box in region:\n",
    "                    box[0], box[1] = box[1], box[0]\n",
    "                    box[2], box[3] = box[3], box[2]\n",
    "                \n",
    "                if np.random.random() > 0.5: # then flip\n",
    "                    fliper = transforms.RandomHorizontalFlip(1)\n",
    "                    img = fliper(img)\n",
    "                    for box in gt: # change gt\n",
    "                        box[0], box[2] = w - box[2], w - box[0]\n",
    "                    for box in region: # ssw generate is [ymin, xmin, ymax, xmax]\n",
    "                        box[0], box[2] = w - box[2], w - box[0]\n",
    "\n",
    "                # then resize\n",
    "                max_side = self.longer_sides[np.random.randint(5)]\n",
    "                if (w > h):\n",
    "                    resizer = transforms.Resize((int(max_side*h/w), max_side))\n",
    "                    ratio = max_side/w\n",
    "                else: # h >= w\n",
    "                    resizer = transforms.Resize((max_side, int(max_side*w/h)))\n",
    "                    ratio = max_side/h\n",
    "                img = resizer(img)\n",
    "                for box in gt:\n",
    "                    box[0] = int(ratio * box[0])\n",
    "                    box[1] = int(ratio * box[1])\n",
    "                    box[2] = int(ratio * box[2])\n",
    "                    box[3] = int(ratio * box[3])\n",
    "                for box in region:\n",
    "                    box[0] = int(ratio * box[0])\n",
    "                    box[1] = int(ratio * box[1])\n",
    "                    box[2] = int(ratio * box[2])\n",
    "                    box[3] = int(ratio * box[3])\n",
    "            else:\n",
    "                raise NotImplementedError(\"This dataset can only be compatible with the paper's implementation\")\n",
    "            print(np.array(region).shape)\n",
    "\n",
    "            \n",
    "            totensor = transforms.ToTensor()\n",
    "            img = totensor(img)\n",
    "            gt = np.array(gt)\n",
    "            gt_box = np.array(gt[:, :4])\n",
    "            \n",
    "            gt_target = gt[:, -1]\n",
    "            target = [0 for _ in range(len(VOC_CLASSES))]\n",
    "            for t in gt_target:\n",
    "                target[t] = 1.0\n",
    "            \n",
    "            gt_target = np.array(target).astype(np.float32)\n",
    "            gt_box = np.array(gt) # split gt -> gt_box,  gt_target\n",
    "            \n",
    "            \n",
    "    \n",
    "            region = np.array(region).astype(np.float32)\n",
    "\n",
    "            region_filter = filter_small_boxes(region, 20)\n",
    "            region = region[region_filter]\n",
    "            \n",
    "            if region_score:\n",
    "                region_score = np.array(region_score)\n",
    "\n",
    "\n",
    "        if \"test\" in self.image_set:\n",
    "            pass\n",
    "        \n",
    "\n",
    "        if region_score is not None:\n",
    "            return img, gt_box, gt_target, region, region_score\n",
    "        else:\n",
    "            return img, gt_box, gt_target, region\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = datasets.VOCDetection(\"~/data/\", '2007', 'trainval', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = VOCDectectionDataset(\"~/data/\", 2007, 'trainval', region_propose='edge_box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6353, 0.6392, 0.6392,  ..., 0.0471, 0.0471, 0.0431],\n",
       "          [0.6314, 0.6353, 0.6353,  ..., 0.0392, 0.0314, 0.0275],\n",
       "          [0.6353, 0.6353, 0.6353,  ..., 0.0510, 0.0588, 0.0667],\n",
       "          ...,\n",
       "          [0.2588, 0.2588, 0.2549,  ..., 0.1333, 0.1373, 0.1412],\n",
       "          [0.2706, 0.2706, 0.2667,  ..., 0.1882, 0.1725, 0.1686],\n",
       "          [0.2706, 0.2667, 0.2627,  ..., 0.2549, 0.2196, 0.1922]],\n",
       " \n",
       "         [[0.7255, 0.7294, 0.7294,  ..., 0.0471, 0.0431, 0.0353],\n",
       "          [0.7333, 0.7333, 0.7333,  ..., 0.0392, 0.0314, 0.0235],\n",
       "          [0.7412, 0.7412, 0.7412,  ..., 0.0549, 0.0627, 0.0706],\n",
       "          ...,\n",
       "          [0.3137, 0.3137, 0.3137,  ..., 0.0392, 0.0353, 0.0353],\n",
       "          [0.3294, 0.3294, 0.3255,  ..., 0.0510, 0.0431, 0.0431],\n",
       "          [0.3294, 0.3255, 0.3216,  ..., 0.0863, 0.0745, 0.0667]],\n",
       " \n",
       "         [[0.7490, 0.7529, 0.7529,  ..., 0.0471, 0.0471, 0.0392],\n",
       "          [0.7569, 0.7608, 0.7608,  ..., 0.0392, 0.0353, 0.0314],\n",
       "          [0.7686, 0.7686, 0.7686,  ..., 0.0588, 0.0667, 0.0824],\n",
       "          ...,\n",
       "          [0.4314, 0.4314, 0.4275,  ..., 0.0196, 0.0196, 0.0157],\n",
       "          [0.4431, 0.4431, 0.4392,  ..., 0.0235, 0.0196, 0.0118],\n",
       "          [0.4431, 0.4392, 0.4353,  ..., 0.0549, 0.0353, 0.0157]]]),\n",
       " array([[242, 290, 326, 466,   8],\n",
       "        [339, 363, 460, 511,   8],\n",
       "        [282, 266, 356, 411,   8]]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([[  1.,  72., 480., 514.],\n",
       "        [  1.,   1., 686., 514.],\n",
       "        [ 16., 176., 466., 514.],\n",
       "        ...,\n",
       "        [129., 238., 167., 310.],\n",
       "        [337., 469., 397., 496.],\n",
       "        [469., 269., 500., 328.]], dtype=float32),\n",
       " array([[0.30461445],\n",
       "        [0.29857314],\n",
       "        [0.28921449],\n",
       "        ...,\n",
       "        [0.02114936],\n",
       "        [0.0211394 ],\n",
       "        [0.02109944]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data.DataLoader(vd, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4)\n",
      "tensor([[[[0.0431, 0.0510, 0.0431,  ..., 0.6431, 0.6392, 0.6353],\n",
      "          [0.0196, 0.0314, 0.0392,  ..., 0.6353, 0.6314, 0.6314],\n",
      "          [0.1216, 0.0863, 0.0627,  ..., 0.6353, 0.6353, 0.6353],\n",
      "          ...,\n",
      "          [0.1294, 0.1333, 0.1216,  ..., 0.2392, 0.2431, 0.2431],\n",
      "          [0.1569, 0.1608, 0.1686,  ..., 0.2588, 0.2706, 0.2706],\n",
      "          [0.1922, 0.2314, 0.2706,  ..., 0.2549, 0.2667, 0.2706]],\n",
      "\n",
      "         [[0.0353, 0.0471, 0.0431,  ..., 0.7333, 0.7294, 0.7255],\n",
      "          [0.0196, 0.0314, 0.0392,  ..., 0.7373, 0.7373, 0.7333],\n",
      "          [0.1333, 0.0902, 0.0667,  ..., 0.7412, 0.7412, 0.7412],\n",
      "          ...,\n",
      "          [0.0392, 0.0510, 0.0471,  ..., 0.2902, 0.2980, 0.2941],\n",
      "          [0.0353, 0.0353, 0.0392,  ..., 0.3176, 0.3294, 0.3294],\n",
      "          [0.0667, 0.0784, 0.0941,  ..., 0.3137, 0.3255, 0.3294]],\n",
      "\n",
      "         [[0.0392, 0.0471, 0.0431,  ..., 0.7569, 0.7529, 0.7490],\n",
      "          [0.0275, 0.0353, 0.0392,  ..., 0.7647, 0.7608, 0.7608],\n",
      "          [0.1451, 0.1020, 0.0706,  ..., 0.7765, 0.7765, 0.7765],\n",
      "          ...,\n",
      "          [0.0235, 0.0431, 0.0275,  ..., 0.4118, 0.4196, 0.4157],\n",
      "          [0.0078, 0.0157, 0.0157,  ..., 0.4314, 0.4431, 0.4431],\n",
      "          [0.0157, 0.0431, 0.0549,  ..., 0.4275, 0.4392, 0.4431]]]])\n",
      "tensor([[[302, 243, 373, 390,   8],\n",
      "         [190, 304, 291, 428,   8],\n",
      "         [277, 223, 339, 344,   8]]])\n",
      "tensor([[[173.,  61., 574., 430.],\n",
      "         [  1.,   1., 574., 430.],\n",
      "         [185., 147., 562., 430.],\n",
      "         ...,\n",
      "         [435., 199., 467., 260.],\n",
      "         [243., 392., 293., 415.],\n",
      "         [156., 225., 183., 275.]]])\n",
      "tensor([[[0.3046],\n",
      "         [0.2986],\n",
      "         [0.2892],\n",
      "         ...,\n",
      "         [0.0211],\n",
      "         [0.0211],\n",
      "         [0.0211]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for img, gt, region, s in dl:\n",
    "    print(img)\n",
    "    print(gt)\n",
    "    print(region)\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,  61., 402., 430.],\n",
       "       [  1.,   1., 574., 430.],\n",
       "       [ 13., 147., 390., 430.],\n",
       "       ...,\n",
       "       [108., 199., 140., 260.],\n",
       "       [282., 392., 332., 415.],\n",
       "       [392., 225., 419., 275.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = region.numpy()[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = filter_small_boxes(x, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,  61., 402., 430.],\n",
       "       [  1.,   1., 574., 430.],\n",
       "       [ 13., 147., 390., 430.],\n",
       "       ...,\n",
       "       [108., 199., 140., 260.],\n",
       "       [282., 392., 332., 415.],\n",
       "       [392., 225., 419., 275.]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2, 1, 2, 1, 10], [2, 3, 1, 2, 10], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 2, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sjjenv",
   "language": "python",
   "name": "sjjenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
